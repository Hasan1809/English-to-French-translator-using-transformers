{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ac832e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from importlib import reload\n",
    "\n",
    "def read_tmx_lxml(file_path):\n",
    "    en_arr , fr_arr = [] , []\n",
    "    for _, elem in etree.iterparse(file_path, tag=\"tu\"):  # Stream parse <tu> tags\n",
    "        en = elem.xpath(\".//tuv[@xml:lang='en']/seg/text()\")[0]\n",
    "        fr = elem.xpath(\".//tuv[@xml:lang='fr']/seg/text()\")[0]\n",
    "        en_arr.append(en)\n",
    "        fr_arr.append(fr)\n",
    "        elem.clear()  # Free memory\n",
    "    return en_arr , fr_arr\n",
    "\n",
    "\n",
    "english_sentences , french_sentences = read_tmx_lxml(r\"data\\en-fr.tmx\\en-fr.tmx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2527d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VÉNUS lN FURS',\n",
       " 'Go on!',\n",
       " '- Tell me first.',\n",
       " 'Howthe hell should I know?',\n",
       " '- Shall I pick it up?',\n",
       " 'Feel like going on a journey.',\n",
       " 'Where to?',\n",
       " \"I haven' t decided.\",\n",
       " 'Come along.',\n",
       " 'We made a deal.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b60d80f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202180, 202180)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences) , len(french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4f085992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training corpus: C:\\Users\\hasan\\AppData\\Local\\Temp\\tmph6cuniee.txt\n",
      "Total sentences: 404360\n",
      "\n",
      "SentencePiece model created:\n",
      "  Model file: translation_bpe_v2.model\n",
      "  Vocab file: translation_bpe_v2.vocab\n",
      "Vocabulary size: 32000\n",
      "Special tokens - PAD: 0, UNK: 1, BOS: 2, EOS: 3\n",
      "English FastText loaded\n",
      "French FastText loaded\n",
      "Vocabulary size: 32000\n",
      "\n",
      "Embedding matrix statistics:\n",
      "  Shape: torch.Size([32000, 300])\n",
      "  Found in English FastText: 4125 (12.9%)\n",
      "  Found in French FastText: 0 (0.0%)\n",
      "  Special tokens (random): 24357 (76.1%)\n",
      "  Not found (random): 3518 (11.0%)\n",
      "  Total FastText coverage: 12.9%\n",
      "  Tokenizer: translation_bpe_v2.model\n",
      "  Vocab size: 32000\n",
      "  Embedding shape: torch.Size([32000, 300])\n",
      "New model created with vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "import tokenizer as tk\n",
    "reload(tk)\n",
    "\n",
    "\n",
    "sp, embedding_matrix = tk.complete_setup(english_sentences, french_sentences)\n",
    "\n",
    "\n",
    "print(f\"New model created with vocab size: {sp.get_piece_size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce566a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: technologie portion repar arrive�dezenfer tuant entendait portion fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers fiers\n"
     ]
    }
   ],
   "source": [
    "import model_arch as ma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "reload(ma)\n",
    "\n",
    "\n",
    "model = ma.create_model(embedding_matrix, sp.get_piece_size())\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "input_sentence = \"Hello, how are you today?\"\n",
    "translation = model.translate(input_sentence, sp, max_length=50)\n",
    "print(f\"Translation: {translation}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6cd1a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 202180 sentence pairs\n",
      "\n",
      "Batch 1:\n",
      "Source batch shape: torch.Size([16, 15])\n",
      "Target batch shape: torch.Size([16, 19])\n",
      "\n",
      "First example:\n",
      "Source tokens: [292, 392, 31957, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
      "Target tokens: [2, 292, 6723, 31940, 292, 680, 31957, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
      "Source text: - what?\n",
      "Target text: - daniel. - quoi?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12637"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_loader as dl\n",
    "reload(dl)\n",
    "\n",
    "dataloader = dl.create_dataloader(\n",
    "    english_sentences, \n",
    "    french_sentences, \n",
    "    sp,  \n",
    "    batch_size=16,\n",
    "    max_length=64, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dl.test_dataloader(dataloader, sp, num_batches=1)\n",
    "\n",
    "\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "58a26668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0000\n",
      "Epoch 0, Batch 0, Loss: 10.3803\n",
      "Epoch 0, Batch 100, Loss: 7.0687\n",
      "Epoch 0, Batch 200, Loss: 5.7968\n",
      "Epoch 0, Batch 300, Loss: 5.2720\n",
      "Epoch 0, Batch 400, Loss: 5.5267\n",
      "Epoch 0, Batch 500, Loss: 5.8053\n",
      "Epoch 0, Batch 600, Loss: 5.0420\n",
      "Epoch 0, Batch 700, Loss: 5.8231\n",
      "Epoch 0, Batch 800, Loss: 4.7605\n",
      "Epoch 0, Batch 900, Loss: 5.0860\n",
      "Epoch 0, Batch 1000, Loss: 4.3438\n",
      "Epoch 0, Batch 1100, Loss: 4.6817\n",
      "Epoch 0, Batch 1200, Loss: 4.8158\n",
      "Epoch 0, Batch 1300, Loss: 4.3287\n",
      "Epoch 0, Batch 1400, Loss: 4.6585\n",
      "Epoch 0, Batch 1500, Loss: 4.8502\n",
      "Epoch 0, Batch 1600, Loss: 4.2405\n",
      "Epoch 0, Batch 1700, Loss: 4.1612\n",
      "Epoch 0, Batch 1800, Loss: 3.7920\n",
      "Epoch 0, Batch 1900, Loss: 4.8153\n",
      "Epoch 0, Batch 2000, Loss: 4.4997\n",
      "Epoch 0, Batch 2100, Loss: 4.6550\n",
      "Epoch 0, Batch 2200, Loss: 4.4531\n",
      "Epoch 0, Batch 2300, Loss: 4.1248\n",
      "Epoch 0, Batch 2400, Loss: 4.5487\n",
      "Epoch 0, Batch 2500, Loss: 4.5681\n",
      "Epoch 0, Batch 2600, Loss: 4.5038\n",
      "Epoch 0, Batch 2700, Loss: 4.4260\n",
      "Epoch 0, Batch 2800, Loss: 4.4351\n",
      "Epoch 0, Batch 2900, Loss: 4.9335\n",
      "Epoch 0, Batch 3000, Loss: 4.4218\n",
      "Epoch 0, Batch 3100, Loss: 4.0647\n",
      "Epoch 0, Batch 3200, Loss: 4.4761\n",
      "Epoch 0, Batch 3300, Loss: 4.7097\n",
      "Epoch 0, Batch 3400, Loss: 4.2840\n",
      "Epoch 0, Batch 3500, Loss: 4.3494\n",
      "Epoch 0, Batch 3600, Loss: 4.0749\n",
      "Epoch 0, Batch 3700, Loss: 3.9254\n",
      "Epoch 0, Batch 3800, Loss: 3.8816\n",
      "Epoch 0, Batch 3900, Loss: 4.7380\n",
      "Epoch 0, Batch 4000, Loss: 4.3568\n",
      "Epoch 0, Batch 4100, Loss: 4.3661\n",
      "Epoch 0, Batch 4200, Loss: 4.1624\n",
      "Epoch 0, Batch 4300, Loss: 3.7404\n",
      "Epoch 0, Batch 4400, Loss: 4.8501\n",
      "Epoch 0, Batch 4500, Loss: 4.3763\n",
      "Epoch 0, Batch 4600, Loss: 4.8154\n",
      "Epoch 0, Batch 4700, Loss: 4.8012\n",
      "Epoch 0, Batch 4800, Loss: 3.9772\n",
      "Epoch 0, Batch 4900, Loss: 3.3049\n",
      "Epoch 0, Batch 5000, Loss: 3.6134\n",
      "Epoch 0, Batch 5100, Loss: 3.9702\n",
      "Epoch 0, Batch 5200, Loss: 4.0359\n",
      "Epoch 0, Batch 5300, Loss: 4.5538\n",
      "Epoch 0, Batch 5400, Loss: 4.6720\n",
      "Epoch 0, Batch 5500, Loss: 3.4794\n",
      "Epoch 0, Batch 5600, Loss: 3.4031\n",
      "Epoch 0, Batch 5700, Loss: 4.2305\n",
      "Epoch 0, Batch 5800, Loss: 3.8022\n",
      "Epoch 0, Batch 5900, Loss: 4.3063\n",
      "Epoch 0, Batch 6000, Loss: 3.7366\n",
      "Epoch 0, Batch 6100, Loss: 3.8980\n",
      "Epoch 0, Batch 6200, Loss: 4.4472\n",
      "Epoch 0, Batch 6300, Loss: 4.5695\n",
      "Epoch 0, Batch 6400, Loss: 4.6788\n",
      "Epoch 0, Batch 6500, Loss: 4.3518\n",
      "Epoch 0, Batch 6600, Loss: 3.7488\n",
      "Epoch 0, Batch 6700, Loss: 4.0404\n",
      "Epoch 0, Batch 6800, Loss: 3.9783\n",
      "Epoch 0, Batch 6900, Loss: 4.3908\n",
      "Epoch 0, Batch 7000, Loss: 3.5720\n",
      "Epoch 0, Batch 7100, Loss: 4.0139\n",
      "Epoch 0, Batch 7200, Loss: 3.7481\n",
      "Epoch 0, Batch 7300, Loss: 5.0844\n",
      "Epoch 0, Batch 7400, Loss: 4.2435\n",
      "Epoch 0, Batch 7500, Loss: 4.3665\n",
      "Epoch 0, Batch 7600, Loss: 4.1430\n",
      "Epoch 0, Batch 7700, Loss: 4.3808\n",
      "Epoch 0, Batch 7800, Loss: 3.5543\n",
      "Epoch 0, Batch 7900, Loss: 4.2026\n",
      "Epoch 0, Batch 8000, Loss: 4.1052\n",
      "Epoch 0, Batch 8100, Loss: 3.4234\n",
      "Epoch 0, Batch 8200, Loss: 3.6775\n",
      "Epoch 0, Batch 8300, Loss: 4.0412\n",
      "Epoch 0, Batch 8400, Loss: 4.3809\n",
      "Epoch 0, Batch 8500, Loss: 3.9989\n",
      "Epoch 0, Batch 8600, Loss: 3.7057\n",
      "Epoch 0, Batch 8700, Loss: 4.0009\n",
      "Epoch 0, Batch 8800, Loss: 3.8026\n",
      "Epoch 0, Batch 8900, Loss: 3.9988\n",
      "Epoch 0, Batch 9000, Loss: 3.7612\n",
      "Epoch 0, Batch 9100, Loss: 4.3829\n",
      "Epoch 0, Batch 9200, Loss: 3.9988\n",
      "Epoch 0, Batch 9300, Loss: 3.5615\n",
      "Epoch 0, Batch 9400, Loss: 3.2367\n",
      "Epoch 0, Batch 9500, Loss: 3.6202\n",
      "Epoch 0, Batch 9600, Loss: 4.1108\n",
      "Epoch 0, Batch 9700, Loss: 4.1674\n",
      "Epoch 0, Batch 9800, Loss: 3.4867\n",
      "Epoch 0, Batch 9900, Loss: 4.1394\n",
      "Epoch 0, Batch 10000, Loss: 4.1311\n",
      "Epoch 0, Batch 10100, Loss: 4.0720\n",
      "Epoch 0, Batch 10200, Loss: 4.2038\n",
      "Epoch 0, Batch 10300, Loss: 3.6473\n",
      "Epoch 0, Batch 10400, Loss: 2.9653\n",
      "Epoch 0, Batch 10500, Loss: 3.2293\n",
      "Epoch 0, Batch 10600, Loss: 4.4372\n",
      "Epoch 0, Batch 10700, Loss: 3.7816\n",
      "Epoch 0, Batch 10800, Loss: 3.5489\n",
      "Epoch 0, Batch 10900, Loss: 3.5092\n",
      "Epoch 0, Batch 11000, Loss: 4.0872\n",
      "Epoch 0, Batch 11100, Loss: 3.4567\n",
      "Epoch 0, Batch 11200, Loss: 3.3330\n",
      "Epoch 0, Batch 11300, Loss: 3.9102\n",
      "Epoch 0, Batch 11400, Loss: 3.8408\n",
      "Epoch 0, Batch 11500, Loss: 4.2078\n",
      "Epoch 0, Batch 11600, Loss: 3.2431\n",
      "Epoch 0, Batch 11700, Loss: 3.9400\n",
      "Epoch 0, Batch 11800, Loss: 3.3364\n",
      "Epoch 0, Batch 11900, Loss: 3.1075\n",
      "Epoch 0, Batch 12000, Loss: 4.1024\n",
      "Epoch 0, Batch 12100, Loss: 3.0830\n",
      "Epoch 0, Batch 12200, Loss: 3.9913\n",
      "Epoch 0, Batch 12300, Loss: 4.0844\n",
      "Epoch 0, Batch 12400, Loss: 3.7347\n",
      "Epoch 0, Batch 12500, Loss: 3.2374\n",
      "Epoch 0, Batch 12600, Loss: 3.4148\n",
      "BLEU Score: 0.0237\n",
      "Epoch 1, Batch 0, Loss: 3.5843\n",
      "Epoch 1, Batch 100, Loss: 3.2699\n",
      "Epoch 1, Batch 200, Loss: 3.4995\n",
      "Epoch 1, Batch 300, Loss: 3.6543\n",
      "Epoch 1, Batch 400, Loss: 3.6840\n",
      "Epoch 1, Batch 500, Loss: 3.5402\n",
      "Epoch 1, Batch 600, Loss: 3.5685\n",
      "Epoch 1, Batch 700, Loss: 3.7892\n",
      "Epoch 1, Batch 800, Loss: 3.4828\n",
      "Epoch 1, Batch 900, Loss: 3.9239\n",
      "Epoch 1, Batch 1000, Loss: 3.6042\n",
      "Epoch 1, Batch 1100, Loss: 2.8604\n",
      "Epoch 1, Batch 1200, Loss: 2.5026\n",
      "Epoch 1, Batch 1300, Loss: 4.2431\n",
      "Epoch 1, Batch 1400, Loss: 3.8177\n",
      "Epoch 1, Batch 1500, Loss: 3.6292\n",
      "Epoch 1, Batch 1600, Loss: 3.1104\n",
      "Epoch 1, Batch 1700, Loss: 3.2178\n",
      "Epoch 1, Batch 1800, Loss: 3.4262\n",
      "Epoch 1, Batch 1900, Loss: 3.8401\n",
      "Epoch 1, Batch 2000, Loss: 3.7384\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hasan\\OneDrive\\Desktop\\AI\\Translator_2\\trainer.py:46\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, num_epochs, device, optimizer, criterion, sp)\u001b[0m\n\u001b[0;32m     43\u001b[0m src_batch \u001b[38;5;241m=\u001b[39m src_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     44\u001b[0m tgt_batch \u001b[38;5;241m=\u001b[39m tgt_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\hasan\\OneDrive\\Desktop\\AI\\Translator_2\\trainer.py:27\u001b[0m, in \u001b[0;36mtrain_step_fixed\u001b[1;34m(model, src_batch, tgt_batch, optimizer, criterion)\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[0;32m     22\u001b[0m     logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),  \u001b[38;5;66;03m# [batch_size * seq_len, vocab_size]\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     tgt_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)                \u001b[38;5;66;03m# [batch_size * seq_len]\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\hasan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hasan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import trainer as tr \n",
    "reload(tr)\n",
    "\n",
    "num_epochs = 2\n",
    "device = 'cuda'\n",
    "\n",
    "tr.train(model , dataloader , num_epochs,device, optimizer, criterion , sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40b71cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "caa8f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "04f42946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.4109\n",
      "1. Translation: comment ça va?\n",
      "   Reference:  comment ça va?\n",
      "2. Translation: comment t'appelles- tu?\n",
      "   Reference:  comment tu t'appelles\n",
      "3. Translation: où vas- tu?\n",
      "   Reference:  bonjour, où vas-tu?\n",
      "4. Translation: je ne vous comprends pas.\n",
      "   Reference:  je suis désolé, je ne vous comprends pas\n",
      "5. Translation: je suis malade.\n",
      "   Reference:  je suis malade aujourd'hui\n",
      "6. Translation: a quelle heure le train part le train?\n",
      "   Reference:  à quelle heure part le train?\n",
      "7. Translation: le chat dort sur le canapé.\n",
      "   Reference:  le chat dort sur le canapé.\n",
      "8. Translation: je t'aime très fort.\n",
      "   Reference:  je t'aime beaucoup.\n",
      "9. Translation: je ne suis pas sûr de pouvoir vous aider.\n",
      "   Reference:  bien que je ne sois pas sûr de pouvoir vous aider.\n",
      "10. Translation: j'essaierai de vous avoir bien d'être le meilleur pour vous.\n",
      "   Reference:  je ferai de mon mieux pour vous aider.\n",
      "11. Translation: aidez- moi à cette tâche.\n",
      "   Reference:  s'il vous plaît, aidez-moi avec cette tâche.\n",
      "12. Translation: pouvez- vous me dire la fois?\n",
      "   Reference:  pouvez-vous me dire l'heure?\n",
      "13. Translation: je dois m'acheter des courses.\n",
      "   Reference:  j'ai besoin d'acheter des courses.\n",
      "14. Translation: où est l'hôpital d'hôpital?\n",
      "   Reference:  où est l'hôpital le plus proche?\n",
      "15. Translation: je voudrais un café.\n",
      "   Reference:  je voudrais une tasse de café.\n",
      "16. Translation: c'est quoi ton livre préf favorite?\n",
      "   Reference:  quel est votre livre préféré?\n",
      "17. Translation: vous aimez voyager?\n",
      "   Reference:  aimez-vous voyager?\n",
      "18. Translation: j'aime bien écouter la musique.\n",
      "   Reference:  j'aime écouter de la musique.\n",
      "19. Translation: pouvez- vous vous présenter un bon film?\n",
      "   Reference:  pouvez-vous recommander un bon film?\n",
      "20. Translation: et votre avis?\n",
      "   Reference:  quel est votre passe-temps?\n",
      "21. Translation: je suis désolé.\n",
      "   Reference:  je suis désolé.\n",
      "22. Translation: je t'en prie.\n",
      "   Reference:  s'il vous plaît, arrêtez\n",
      "23. Translation: je ne suis pas sûr de ce que vous voulez dire.\n",
      "   Reference:   je ne suis pas sûr de ce que vous voulez dire.\n",
      "24. Translation: bonjour.\n",
      "   Reference:  bonjour.\n",
      "25. Translation: bientôt.\n",
      "   Reference:  prompt rétabli bientôt.\n"
     ]
    }
   ],
   "source": [
    "import bleu_tester as bt\n",
    "reload(bt)\n",
    "\n",
    "source_sentences , reference_sentences = bt.get_sentences()\n",
    "\n",
    "results = bt.calculate_bleu_score(model, sp, source_sentences, reference_sentences)\n",
    "\n",
    "print(f\"BLEU Score: {results['bleu_score']:.4f}\")\n",
    "\n",
    "for i, (trans, ref) in enumerate(zip(results['translations'], results['references'])):\n",
    "    print(f\"{i+1}. Translation: {trans}\")\n",
    "    print(f\"   Reference:  {ref}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
