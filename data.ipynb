{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ac832e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from importlib import reload\n",
    "\n",
    "def read_tmx_lxml(file_path):\n",
    "    en_arr , fr_arr = [] , []\n",
    "    for _, elem in etree.iterparse(file_path, tag=\"tu\"):  # Stream parse <tu> tags\n",
    "        en = elem.xpath(\".//tuv[@xml:lang='en']/seg/text()\")[0]\n",
    "        fr = elem.xpath(\".//tuv[@xml:lang='fr']/seg/text()\")[0]\n",
    "        en_arr.append(en)\n",
    "        fr_arr.append(fr)\n",
    "        elem.clear()  # Free memory\n",
    "    return en_arr , fr_arr\n",
    "\n",
    "\n",
    "english_sentences , french_sentences = read_tmx_lxml(r\"data\\en-fr.tmx\\en-fr.tmx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2527d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VÉNUS lN FURS',\n",
       " 'Go on!',\n",
       " '- Tell me first.',\n",
       " 'Howthe hell should I know?',\n",
       " '- Shall I pick it up?',\n",
       " 'Feel like going on a journey.',\n",
       " 'Where to?',\n",
       " \"I haven' t decided.\",\n",
       " 'Come along.',\n",
       " 'We made a deal.']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b60d80f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202180, 202180)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences) , len(french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def modify_dataset_punctuation(english_sentences, french_sentences, seed=42):\n",
    "    random.seed(seed)  # For reproducibility\n",
    "    \n",
    "    # Create indices for half the dataset\n",
    "    total_sentences = len(english_sentences)\n",
    "    indices_to_modify = random.sample(range(total_sentences), total_sentences // 2)\n",
    "    \n",
    "    print(f\"Total sentences: {total_sentences:,}\")\n",
    "    print(f\"Modifying punctuation for: {len(indices_to_modify):,} sentences\")\n",
    "    \n",
    "    # Modify the selected sentences\n",
    "    english_modified = english_sentences.copy()\n",
    "    french_modified = french_sentences.copy()\n",
    "    \n",
    "    for idx in indices_to_modify:\n",
    "        # Remove ending punctuation from both languages\n",
    "        english_modified[idx] = english_sentences[idx].rstrip('.!?;:').strip()\n",
    "        french_modified[idx] = french_sentences[idx].rstrip('.!?;:').strip()\n",
    "    \n",
    "    return english_modified, french_modified, indices_to_modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f7bd8572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 202,180\n",
      "Modifying punctuation for: 101,090 sentences\n"
     ]
    }
   ],
   "source": [
    "english_modified , french_modified , _= modify_dataset_punctuation(english_sentences, french_sentences,42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3db0f615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202180"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d27b8149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 181,962 pairs\n",
      "Testing: 20,218 pairs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_english, test_english, train_french, test_french = train_test_split(\n",
    "   english_sentences, french_sentences, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(train_english):,} pairs\")\n",
    "print(f\"Testing: {len(test_english):,} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4f085992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training corpus: C:\\Users\\hasan\\AppData\\Local\\Temp\\tmpud8e4f20.txt\n",
      "Total sentences: 363924\n",
      "\n",
      "SentencePiece model created:\n",
      "  Model file: translation_bpe.model\n",
      "  Vocab file: translation_bpe.vocab\n",
      "Vocabulary size: 32000\n",
      "Special tokens - PAD: 0, UNK: 1, BOS: 2, EOS: 3\n",
      "English FastText loaded\n",
      "French FastText loaded\n",
      "Vocabulary size: 32000\n",
      "\n",
      "Embedding matrix statistics:\n",
      "  Shape: torch.Size([32000, 300])\n",
      "  Found in English FastText: 4262 (13.3%)\n",
      "  Found in French FastText: 0 (0.0%)\n",
      "  Special tokens (random): 24059 (75.2%)\n",
      "  Not found (random): 3679 (11.5%)\n",
      "  Total FastText coverage: 13.3%\n",
      "  Tokenizer: translation_bpe.model\n",
      "  Vocab size: 32000\n",
      "  Embedding shape: torch.Size([32000, 300])\n",
      "New model created with vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "import tokenizer as tk\n",
    "reload(tk)\n",
    "\n",
    "\n",
    "sp, embedding_matrix = tk.complete_setup(train_english, train_french)\n",
    "\n",
    "\n",
    "print(f\"New model created with vocab size: {sp.get_piece_size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8ce566a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: lax lax lax lax lax lax lax lax lax lax lax hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles hostiles lax lax lax lax lax lax lax lax lax hostiles hostiles hostiles hostiles hostiles hostiles lax lax lax lax lax lax\n"
     ]
    }
   ],
   "source": [
    "import model_arch as ma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "reload(ma)\n",
    "\n",
    "\n",
    "model = ma.create_model(embedding_matrix, sp.get_piece_size())\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "input_sentence = \"Hello, how are you today?\"\n",
    "translation = model.translate(input_sentence, sp, max_length=50)\n",
    "print(f\"Translation: {translation}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6cd1a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 181962 sentence pairs\n",
      "\n",
      "Batch 1:\n",
      "Source batch shape: torch.Size([16, 22])\n",
      "Target batch shape: torch.Size([16, 18])\n",
      "\n",
      "First example:\n",
      "Source tokens: [353, 741, 290, 4343, 358, 23342, 451, 634, 2870, 1396, 337, 31940, 3, 0, 0, 0, 0, 0, 0, 0]...\n",
      "Target tokens: [2, 1129, 274, 31947, 7319, 25872, 2085, 546, 536, 1673, 2870, 31940, 3, 0, 0, 0, 0, 0]...\n",
      "Source text: and then the electrician with his machine came in.\n",
      "Target text: puis l'électricien arrive avec sa petite machine.\n",
      "Dataset created with 20218 sentence pairs\n",
      "\n",
      "Batch 1:\n",
      "Source batch shape: torch.Size([16, 23])\n",
      "Target batch shape: torch.Size([16, 27])\n",
      "\n",
      "First example:\n",
      "Source tokens: [480, 448, 945, 1635, 31940, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
      "Target tokens: [2, 426, 13845, 31960, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
      "Source text: they are savages.\n",
      "Target text: des sauvages!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1264"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import data_loader as dl\n",
    "reload(dl)\n",
    "\n",
    "train_loader = dl.create_dataloader(\n",
    "    train_english, \n",
    "    train_french, \n",
    "    sp,  \n",
    "    batch_size=16,\n",
    "    max_length=64, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dl.test_dataloader(train_loader, sp, num_batches=1)\n",
    "\n",
    "test_loader = dl.create_dataloader(\n",
    "    test_english, \n",
    "    test_french, \n",
    "    sp,  \n",
    "    batch_size=16,\n",
    "    max_length=64, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "dl.test_dataloader(test_loader , sp , num_batches = 1)\n",
    "\n",
    "len(train_loader)\n",
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a26668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Batch 0, Training Loss: 2.4290\n",
      "Batch 1000, Training Loss: 2.9564\n",
      "Batch 2000, Training Loss: 2.4287\n",
      "Batch 3000, Training Loss: 2.7509\n",
      "Batch 4000, Training Loss: 2.9753\n",
      "Batch 5000, Training Loss: 2.9637\n",
      "Batch 6000, Training Loss: 2.3331\n"
     ]
    }
   ],
   "source": [
    "import trainer as tr \n",
    "reload(tr)\n",
    "\n",
    "num_epochs = 5\n",
    "device = 'cuda'\n",
    "\n",
    "tr.train(model , train_loader ,test_loader,  num_epochs, device, optimizer, criterion , sp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "40b71cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "04f42946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.2155\n",
      "1. Translation: comment allez- vous?\n",
      "   Reference:  comment ça va?\n",
      "2. Translation: comment s'appelle- t- il?\n",
      "   Reference:  comment tu t'appelles\n",
      "3. Translation: bonjour, où vas- tu?\n",
      "   Reference:  bonjour, où vas-tu?\n",
      "4. Translation: je suis désolé, je ne comprends pas.\n",
      "   Reference:  je suis désolé, je ne vous comprends pas\n",
      "5. Translation: je suis malade aujourd'hui.\n",
      "   Reference:  je suis malade aujourd'hui\n",
      "6. Translation: quelle heure fait la partie de la part?\n",
      "   Reference:  à quelle heure part le train?\n",
      "7. Translation: le chat est sur le toit.\n",
      "   Reference:  le chat dort sur le canapé.\n",
      "8. Translation: je t'aime beaucoup.\n",
      "   Reference:  je t'aime beaucoup.\n",
      "9. Translation: mais je ne suis pas sûr si je peux vous aider.\n",
      "   Reference:  bien que je ne sois pas sûr de pouvoir vous aider.\n",
      "10. Translation: je vais essayer de vous avoir dérang.\n",
      "   Reference:  je ferai de mon mieux pour vous aider.\n",
      "11. Translation: aidez- moi avec cette attaque.\n",
      "   Reference:  s'il vous plaît, aidez-moi avec cette tâche.\n",
      "12. Translation: dis- moi le temps?\n",
      "   Reference:  pouvez-vous me dire l'heure?\n",
      "13. Translation: il faut que je m'occupe de mes frais.\n",
      "   Reference:  j'ai besoin d'acheter des courses.\n",
      "14. Translation: où est la plus proche de l'hôpital?\n",
      "   Reference:  où est l'hôpital le plus proche?\n",
      "15. Translation: j'aimerais un café.\n",
      "   Reference:  je voudrais une tasse de café.\n",
      "16. Translation: quel est ton livre préféré?\n",
      "   Reference:  quel est votre livre préféré?\n",
      "17. Translation: vous voulez vous aider?\n",
      "   Reference:  aimez-vous voyager?\n",
      "18. Translation: j'espère que j'essaie de chanter.\n",
      "   Reference:  j'aime écouter de la musique.\n",
      "19. Translation: vous pouvez vous servir un bon film?\n",
      "   Reference:  pouvez-vous recommander un bon film?\n",
      "20. Translation: quel est ton sac?\n",
      "   Reference:  quel est votre passe-temps?\n",
      "21. Translation: je suis désolé.\n",
      "   Reference:  je suis désolé.\n",
      "22. Translation: arrête de t'en prie.\n",
      "   Reference:  s'il vous plaît, arrêtez\n",
      "23. Translation: je ne suis pas sûr de ce que vous voulez dire.\n",
      "   Reference:   je ne suis pas sûr de ce que vous voulez dire.\n",
      "24. Translation: bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour bonjour\n",
      "   Reference:  bonjour.\n",
      "25. Translation: va bientôt bientôt.\n",
      "   Reference:  prompt rétabli bientôt.\n"
     ]
    }
   ],
   "source": [
    "import bleu_tester as bt\n",
    "reload(bt)\n",
    "\n",
    "source_sentences , reference_sentences = bt.get_sentences()\n",
    "\n",
    "results = bt.calculate_bleu_score(model, sp, source_sentences, reference_sentences)\n",
    "\n",
    "print(f\"BLEU Score: {results['bleu_score']:.4f}\")\n",
    "\n",
    "for i, (trans, ref) in enumerate(zip(results['translations'], results['references'])):\n",
    "    print(f\"{i+1}. Translation: {trans}\")\n",
    "    print(f\"   Reference:  {ref}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
